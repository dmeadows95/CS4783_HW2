{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWMvBAruKoWJawpwUEHl1/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmeadows95/CS4783_HW2/blob/main/CS4783_HW2_DamonMeadows.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "SIMoRbiS6loY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define activation function\n",
        "def sigmoid(z):\n",
        "\treturn 1/(1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "  return sigmoid(z)*(1-sigmoid(z))"
      ],
      "metadata": {
        "id": "B8CfpbtJUAe7"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "x_train = np.loadtxt(\"X_train.csv\") \n",
        "y_train = np.loadtxt(\"Y_train.csv\")\n",
        "x_test = np.loadtxt(\"X_test.csv\") \n",
        "y_test = np.loadtxt(\"Y_test.csv\")  \n",
        "\n",
        "# Define initial parameters\n",
        "# 10 neurons in hidden layer, 2 features from input\n",
        "weights_1 = np.random.random((10,2))\n",
        "biases_1 = []\n",
        "# 10 biases for 10 neurons\n",
        "for i in range (0, 10): \n",
        "  n = random.random()\n",
        "  biases_1.append(n)\n",
        "# reshape biases for matrix calculations\n",
        "biases_1 = np.array(biases_1)\n",
        "biases_1 = biases_1.reshape((10,1))\n",
        "# 1 neuron in output layer, b/c regression\n",
        "# 10 inputs from previous layer\n",
        "weights_2 = np.random.random((1,10))\n",
        "# 1 bias for 1 output neuron\n",
        "biases_2 = random.random()\n",
        "# reshape bias for matrix calculations\n",
        "biases_2 = np.array(biases_2)\n",
        "biases_2 = biases_2.reshape((1,1))\n",
        "\n",
        "learning_rate = .01\n"
      ],
      "metadata": {
        "id": "0ojbUo3-UKYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69119add-ae86-4fc9-deaf-56fd2e37496b"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fwd Pass\n",
        "def feed_forward(x, weights_1, biases_1, weights_2, biases_2):\n",
        "  # hidden layer, sigmoid activation\n",
        "  z1 = np.dot(x, weights_1.T) + biases_1.T\n",
        "  a1 = sigmoid(z1)\n",
        "\n",
        "  # output layer, linear activation (no function necessary)\n",
        "  z2 = np.dot(a1, weights_2.T) + biases_2\n",
        "  a2 = z2\n",
        "  return a2\n"
      ],
      "metadata": {
        "id": "0C-at5zOaFBx"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "def mse_loss(output, target):\n",
        "  loss = (np.square(output - target))\n",
        "  loss = np.sum(loss) / len(target)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "UEv29HX6_eRH"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation\n",
        "\n",
        "def back_prop(x, y, weights_1, biases_1, weights_2, biases_2, alpha):\n",
        "# hidden layer, sigmoid activation\n",
        "  z1 = np.dot(x, weights_1.T) + biases_1.T\n",
        "  a1 = sigmoid(z1)\n",
        "\n",
        "  # output layer, linear activation (no function necessary)\n",
        "  z2 = np.dot(a1, weights_2.T) + biases_2\n",
        "  a2 = z2\n",
        "  \n",
        "# Compute gradients\n",
        "  y = y.reshape((100,1))\n",
        "  dz2 = (a2 - y)\n",
        "  dw2 = np.dot(dz2.T, a1)\n",
        "  db2 = dz2\n",
        "  dz1 = np.dot(weights_2.T, dz2.T) * sigmoid(z1.T) * (1-sigmoid(z1.T))\n",
        "  dw1 = np.dot(dz1, x)\n",
        "  db1 = dz1\n",
        "\n",
        "# Update weights\n",
        "  new_weights_1 = weights_1 - dw1 * alpha\n",
        "  new_weights_2 = weights_2 - dw2 * alpha\n",
        "  new_biases_1 = biases_1 - db1 * alpha\n",
        "  new_biases_2 = biases_2 - db2 * alpha\n",
        "\n",
        "  return(new_weights_1, new_weights_2, new_biases_1, new_biases_2)\n"
      ],
      "metadata": {
        "id": "r3rSzmGxgprd"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(x, y, weights_1, biases_1, weights_2, biases_2, alpha, epochs):\n",
        "  i = 0\n",
        "  while i < epochs:\n",
        "    output = feed_forward(x, weights_1, biases_1, weights_2, biases_2)\n",
        "    weights_1, weights_2, biases_1, biases_2 = back_prop(x, y, weights_1, biases_1, weights_2, biases_2, alpha)\n",
        "    i += 1\n",
        "\n",
        "  return(weights_1, biases_1, weights_2, biases_2)\n",
        "\n"
      ],
      "metadata": {
        "id": "JEtZp-AElyBk"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_output = feed_forward(x_train, weights_1, biases_1, weights_2, biases_2)\n",
        "initial_loss = mse_loss(initial_output, y_train)\n",
        "\n",
        "trained_w1, trained_b1, trained_w2, trained_b2 = train_loop(x_train, y_train, weights_1, biases_1, weights_2, biases_2, learning_rate, 10)\n",
        "trained_output = feed_forward(x_train, trained_w1, trained_b1, trained_w2, trained_b2)\n",
        "trained_loss = mse_loss(trained_output, y_train)\n",
        "\n",
        "print(initial_loss)\n",
        "print(trained_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OIYiz0MsRZk",
        "outputId": "8d342f63-1f7a-4779-c628-3a0698d56614"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "566798.0115642048\n",
            "112902837626.39355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2.1)\n",
        "The activation function for the output layer should be the linear activation function as the task is regression. This is equivalent to having no activation function.\n",
        "\n",
        "Question 2.2)\n",
        "Their should be one neuron in the output layer because the task is regression."
      ],
      "metadata": {
        "id": "qzMahXFnwT_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjxrvwYV0g7m",
        "outputId": "411e9054-e386-42d7-8e36-41a5102510cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    }
  ]
}